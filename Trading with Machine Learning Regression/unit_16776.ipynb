{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b32a96",
   "metadata": {
    "id": "46b32a96"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32a8a0",
   "metadata": {
    "id": "fb32a8a0"
   },
   "source": [
    "## Data Pre-processing \n",
    "In the previous notebook, you learned about creating custom input parameters that you can use in your model.  \n",
    "\n",
    "In this notebook, you will learn about a data preprocessing step known as scaling. Also, you will learn how to divide your data into input and output datasets. These are known as independent (X) and dependent (Y) variables. The key steps involved here are:\n",
    "\n",
    "1. [Import the Data](#import)\n",
    "2. [Check and Drop NaN Values](#drop)\n",
    "3. [Scale the Data](#scale)\n",
    "4. [Create Independent and Dependent Variables](#xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c92ca44",
   "metadata": {
    "id": "5c92ca44"
   },
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To ignore unwanted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a29a2",
   "metadata": {
    "id": "e67a29a2"
   },
   "source": [
    "<a id='import'></a>\n",
    "## Import the Data\n",
    "\n",
    "We will use the dataset with the input and output parameters that we created in the previous notebook. The input data is stored in `input_parameters.csv`, which we will import here as `gold_prices` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9396590b",
   "metadata": {
    "id": "9396590b",
    "outputId": "ce25e0da-1724-4c57-c70d-a2751f3edd40"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "gold_prices = pd.read_csv(\n",
    "    '../data_modules/input_parameters.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc3df2",
   "metadata": {},
   "source": [
    "<a id='drop'></a>\n",
    "## Check and Drop NaN Values\n",
    "\n",
    "We will check for any `NaN` values present in the `gold_prices` dataset using the `isna` method. Then, we will use the `sum` method to find the total number of `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532a33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open      0\n",
       "High      0\n",
       "Low       0\n",
       "Close     0\n",
       "S_3       3\n",
       "S_15     15\n",
       "S_60     60\n",
       "Corr     13\n",
       "Std_U     0\n",
       "Std_D     0\n",
       "OD        1\n",
       "OL        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "gold_prices.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e5fc1",
   "metadata": {},
   "source": [
    "As you can see, there are several `NaN` values in the dataset as a result of the new input parameters. However, we will simply drop all these `NaN` values using `dropna`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deea52b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open     0\n",
       "High     0\n",
       "Low      0\n",
       "Close    0\n",
       "S_3      0\n",
       "S_15     0\n",
       "S_60     0\n",
       "Corr     0\n",
       "Std_U    0\n",
       "Std_D    0\n",
       "OD       0\n",
       "OL       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all the NaN values\n",
    "gold_prices.dropna(inplace=True)\n",
    "\n",
    "# Check for NaN values\n",
    "gold_prices.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd9de",
   "metadata": {
    "id": "813bd9de"
   },
   "source": [
    "We can see that all the `NaN` values have been removed from our dataset.\n",
    "\n",
    "<a id='scale'></a>\n",
    "## Scale the Data\n",
    "\n",
    "Suppose our dataset has feature values which have high differences from each other. In that case, features with greater magnitude might dominate over other observations. This could lead to the regression model not being able to learn from all of the features correctly. \n",
    "\n",
    "To avoid this, we will standardise the dataset by centring and scaling it. Centring reduces the mean value of the features to 0. Scaling refers to dividing each entry by the standard deviation of the data. This transforms the standard deviation of the features to 1.\n",
    "\n",
    "We will use the `StandardScaler()` function to standardise our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9f775a",
   "metadata": {
    "id": "ef9f775a",
    "outputId": "2c64b70c-dbb7-41fc-e80f-62b03e04b311"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>S_3</th>\n",
       "      <th>S_15</th>\n",
       "      <th>S_60</th>\n",
       "      <th>Corr</th>\n",
       "      <th>Std_U</th>\n",
       "      <th>Std_D</th>\n",
       "      <th>OD</th>\n",
       "      <th>OL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-07-10</th>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.388435</td>\n",
       "      <td>0.264553</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.013875</td>\n",
       "      <td>0.444063</td>\n",
       "      <td>2.223760</td>\n",
       "      <td>-1.247226</td>\n",
       "      <td>1.480757</td>\n",
       "      <td>0.216553</td>\n",
       "      <td>0.308280</td>\n",
       "      <td>0.675468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11</th>\n",
       "      <td>0.751969</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.704618</td>\n",
       "      <td>0.747942</td>\n",
       "      <td>0.159572</td>\n",
       "      <td>0.327314</td>\n",
       "      <td>2.195139</td>\n",
       "      <td>-2.894026</td>\n",
       "      <td>-0.745596</td>\n",
       "      <td>0.730431</td>\n",
       "      <td>3.103464</td>\n",
       "      <td>4.243061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-12</th>\n",
       "      <td>0.639286</td>\n",
       "      <td>0.684325</td>\n",
       "      <td>0.681697</td>\n",
       "      <td>0.731221</td>\n",
       "      <td>0.400532</td>\n",
       "      <td>0.261003</td>\n",
       "      <td>2.171491</td>\n",
       "      <td>-2.296600</td>\n",
       "      <td>0.630713</td>\n",
       "      <td>-0.587816</td>\n",
       "      <td>-0.739927</td>\n",
       "      <td>-0.928662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-15</th>\n",
       "      <td>0.724560</td>\n",
       "      <td>0.697981</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>0.738822</td>\n",
       "      <td>0.579342</td>\n",
       "      <td>0.266537</td>\n",
       "      <td>2.147346</td>\n",
       "      <td>-0.100679</td>\n",
       "      <td>-0.320568</td>\n",
       "      <td>-0.498424</td>\n",
       "      <td>0.557849</td>\n",
       "      <td>-0.068840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-16</th>\n",
       "      <td>0.828106</td>\n",
       "      <td>0.822406</td>\n",
       "      <td>0.836026</td>\n",
       "      <td>0.846741</td>\n",
       "      <td>0.743888</td>\n",
       "      <td>0.257452</td>\n",
       "      <td>2.119389</td>\n",
       "      <td>0.310616</td>\n",
       "      <td>-0.037213</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>0.677638</td>\n",
       "      <td>0.739637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close       S_3      S_15  \\\n",
       "Date                                                                     \n",
       "2013-07-10  0.278400  0.388435  0.264553  0.247863  0.013875  0.444063   \n",
       "2013-07-11  0.751969  0.693429  0.704618  0.747942  0.159572  0.327314   \n",
       "2013-07-12  0.639286  0.684325  0.681697  0.731221  0.400532  0.261003   \n",
       "2013-07-15  0.724560  0.697981  0.761153  0.738822  0.579342  0.266537   \n",
       "2013-07-16  0.828106  0.822406  0.836026  0.846741  0.743888  0.257452   \n",
       "\n",
       "                S_60      Corr     Std_U     Std_D        OD        OL  \n",
       "Date                                                                    \n",
       "2013-07-10  2.223760 -1.247226  1.480757  0.216553  0.308280  0.675468  \n",
       "2013-07-11  2.195139 -2.894026 -0.745596  0.730431  3.103464  4.243061  \n",
       "2013-07-12  2.171491 -2.296600  0.630713 -0.587816 -0.739927 -0.928662  \n",
       "2013-07-15  2.147346 -0.100679 -0.320568 -0.498424  0.557849 -0.068840  \n",
       "2013-07-16  2.119389  0.310616 -0.037213 -0.073920  0.677638  0.739637  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the data in gold_prices and store it as an array in variable scaled\n",
    "scaled = scaler.fit_transform(gold_prices)\n",
    "\n",
    "# Convert data stored in scaled from array to dataframe\n",
    "scaled_gold_prices = pd.DataFrame(\n",
    "    scaled, index=gold_prices.index, columns=gold_prices.columns)\n",
    "\n",
    "# Print the new dataframe\n",
    "scaled_gold_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3bc35",
   "metadata": {},
   "source": [
    "It should be noted that the complete data shouldn't be scaled before splitting into train and test datasets. The correct approach is to fit the scaler on the train data and use the fitted scaler model to transform the train and test sets. This avoids data leakage from the test set to the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3db6e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we learned how to check, drop NaN values and scale the data. <br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing and Prediction.ipynb",
   "provenance": [
    {
     "file_id": "1oE_u21fmFtV9vYYfPBMW3rjuH629d5rw",
     "timestamp": 1644837511413
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
