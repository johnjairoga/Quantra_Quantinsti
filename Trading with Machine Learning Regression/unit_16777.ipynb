{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b32a96",
   "metadata": {
    "id": "46b32a96"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32a8a0",
   "metadata": {
    "id": "fb32a8a0"
   },
   "source": [
    "## Concept of Pipeline\n",
    "In the previous notebooks, we have covered the concepts of importing data and creating input and output parameters. We also learnt about scaling which is a data preprocessing method used for preparing the data for a machine learning model. \n",
    "\n",
    "In this notebook, we will take a diversion from the GLD dataset we have been working on to learn about the concept of a pipeline. Later in this notebook, will also test the pipeline we create on a sample dataset.\n",
    "\n",
    "A pipeline in Python is used to execute a certain number of steps sequentially. This is extremely useful when you need to jump through the different steps of data processing and finally train the machine learning model or use the model to make predictions. The key steps involved are:\n",
    "\n",
    "1. [Create a Sample Dataset](#sample)\n",
    "2. [Create Pipeline](#pipeline)\n",
    "3. [Test Pipeline](#test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c92ca44",
   "metadata": {
    "id": "5c92ca44"
   },
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# To ignore unwanted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a29a2",
   "metadata": {
    "id": "e67a29a2"
   },
   "source": [
    "<a id='sample'></a>\n",
    "## Create a Sample Dataset\n",
    "\n",
    "Let us first create a sample dataset that we will use to test our pipeline. We will create 2 lists `x` and `y` which contain close prices of Tesla and Amazon respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9396590b",
   "metadata": {
    "id": "9396590b",
    "outputId": "ce25e0da-1724-4c57-c70d-a2751f3edd40"
   },
   "outputs": [],
   "source": [
    "# List containing close prices of Tesla (independent variable)\n",
    "x= [663.90, 674.90, 628.16, 658.80, 707.73, 759.63, 758.26, 740.37, 775.00, 703.55]\n",
    "\n",
    "# List containing close prices of Amazon (dependent variable)\n",
    "y= [2151.82, 2151.14, 2082.00, 2135.50, 2221.55, 2302.93, 2404.19, 2433.68, 2510.22, 2447.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c541e",
   "metadata": {},
   "source": [
    "Next, we will split our sample data into test and train datasets. We will use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a987b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the splitting length \n",
    "split=int(0.8*len(x))\n",
    "\n",
    "# Split x into train and test sets\n",
    "x_train = x[:split]\n",
    "x_test = x[split:]\n",
    "\n",
    "# Split y into train and test sets\n",
    "y_train = y[:split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6afdf0",
   "metadata": {},
   "source": [
    "Now, we will reshape our array from a 1D array to a 2D array. This is because machine learning methods take 2D array of values as input for prediction. Each item in the array is a \"point\" you want your model to predict. So, here the input is a 2D array of shape `(-1,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9001839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training data\n",
    "x_train = np.reshape(x_train, (-1, 1))\n",
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "\n",
    "# Reshape testing data\n",
    "x_test = np.reshape(x_test, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6648cc",
   "metadata": {
    "id": "7a6648cc"
   },
   "source": [
    "<a id='pipeline'></a>\n",
    "## Pipeline\n",
    "As mentioned before, a pipeline allows us to execute a certain number of steps sequentially. It may contain several transformation steps followed by the final estimation step. \n",
    "\n",
    "This is useful when we have a fixed sequence of steps in processing the data. Some codes are meant to transform our data. For example, dealing with missing values in the data and standardising the data. These steps are known as transformers. Other codes are meant to predict variables by fitting an algorithm such as linear regression, random forest or support vector machine (SVM). These are called estimators. \n",
    "\n",
    "So, in a pipeline, we first sequentially apply a list of transformers (data processing) and then a final estimator (ML model). \n",
    "\n",
    "The code shown below depicts these steps. We store the list of tuples in the variable `steps` sequentially.\n",
    "\n",
    "Syntax: \n",
    "```python\n",
    "steps = [(name_1,transform_1), (name_2,transform_2),........(name_n,transform_n)]\n",
    "Pipeline(steps)\n",
    "```\n",
    "We are using the following two steps in our pipeline:\n",
    "1. Scale the data \n",
    "3. Fit the data using the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280dcb67",
   "metadata": {
    "id": "280dcb67"
   },
   "outputs": [],
   "source": [
    "# First we put scaling and then linear regression in the pipeline.\n",
    "steps = [('scaler', StandardScaler()), \n",
    "         ('linear', LinearRegression())]\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc15e0",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "## Test Pipeline\n",
    "After creating a pipeline, we will fit it on our training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1656a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing linear, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('linear', LinearRegression())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fac12",
   "metadata": {},
   "source": [
    "Let us now predict the values for `y` using the `predict()` function with the parameter as `x_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a30fdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2418.75009388],\n",
       "       [2246.40191606]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3db6e",
   "metadata": {},
   "source": [
    "As you can see, the pipeline has predicted the `y` values as shown above.  \n",
    "## Conculsion\n",
    "In this notebook, we learned how to create a pipeline. We also tested our pipeline on a sample dataset to see its results. In the next notebook, we will use a pipeline to transform and estimate our GLD dataset.<br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing and Prediction.ipynb",
   "provenance": [
    {
     "file_id": "1oE_u21fmFtV9vYYfPBMW3rjuH629d5rw",
     "timestamp": 1644837511413
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
