{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b32a96",
   "metadata": {
    "id": "46b32a96"
   },
   "source": [
    " # Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32a8a0",
   "metadata": {
    "id": "fb32a8a0"
   },
   "source": [
    "## Cross Validation, Test and Train\n",
    "In the previous notebooks, you have learned how to import data, preprocess data and create custom input and output parameters for your model. You were also introduced to the concept of pipeline which we will now use on our GLD dataset.\n",
    "\n",
    "In this notebook, our primary focus will be to find the best hyperparameters for our linear regression model.\n",
    "\n",
    "<b>But what are hyperparameters? And how can we find the best set of hyperparameters for our model?</b>\n",
    "\n",
    "Hyperparameters are parameters that the model cannot estimate itself. These need to be set manually by the user to help in the estimation of the model. We will learn more about hyperparameters and how to evaluate the best performing hyperparameters using cross-validation in this notebook. \n",
    "\n",
    "We will also learn how to split our data into train and test datasets. The key steps are:\n",
    "\n",
    "1. [Import the Data](#import)\n",
    "2. [Create Input and Output Variables](#xy)\n",
    "3. [Data Preprocessing and Hyperparameters](#preprocess)\n",
    "4. [Split Train and Test Data](#split)\n",
    "5. [Grid Search Cross-Validation](#cross)\n",
    "6. [Grid Search Performance](#perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c92ca44",
   "metadata": {
    "id": "5c92ca44"
   },
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# To ignore unwanted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a29a2",
   "metadata": {
    "id": "e67a29a2"
   },
   "source": [
    "<a id='import'></a>\n",
    "## Import the Data\n",
    "\n",
    "The input data is stored in `data_preprocess.csv`, which we will load here and store within the variable `gold_prices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9396590b",
   "metadata": {
    "id": "9396590b",
    "outputId": "ce25e0da-1724-4c57-c70d-a2751f3edd40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>S_3</th>\n",
       "      <th>S_15</th>\n",
       "      <th>S_60</th>\n",
       "      <th>Corr</th>\n",
       "      <th>Std_U</th>\n",
       "      <th>Std_D</th>\n",
       "      <th>OD</th>\n",
       "      <th>OL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-07-10</th>\n",
       "      <td>121.150002</td>\n",
       "      <td>122.349998</td>\n",
       "      <td>120.589996</td>\n",
       "      <td>120.949997</td>\n",
       "      <td>119.406667</td>\n",
       "      <td>122.149334</td>\n",
       "      <td>132.900000</td>\n",
       "      <td>-0.042512</td>\n",
       "      <td>1.199996</td>\n",
       "      <td>0.560006</td>\n",
       "      <td>0.310006</td>\n",
       "      <td>0.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-11</th>\n",
       "      <td>124.260002</td>\n",
       "      <td>124.360001</td>\n",
       "      <td>123.470001</td>\n",
       "      <td>124.239998</td>\n",
       "      <td>120.360001</td>\n",
       "      <td>121.404000</td>\n",
       "      <td>132.727334</td>\n",
       "      <td>-0.680947</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.790001</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>3.310005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-12</th>\n",
       "      <td>123.519997</td>\n",
       "      <td>124.300003</td>\n",
       "      <td>123.320000</td>\n",
       "      <td>124.129997</td>\n",
       "      <td>121.936666</td>\n",
       "      <td>120.980667</td>\n",
       "      <td>132.584667</td>\n",
       "      <td>-0.449336</td>\n",
       "      <td>0.780006</td>\n",
       "      <td>0.199997</td>\n",
       "      <td>-0.740005</td>\n",
       "      <td>-0.720001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-15</th>\n",
       "      <td>124.080002</td>\n",
       "      <td>124.389999</td>\n",
       "      <td>123.839996</td>\n",
       "      <td>124.180000</td>\n",
       "      <td>123.106664</td>\n",
       "      <td>121.016000</td>\n",
       "      <td>132.439000</td>\n",
       "      <td>0.401985</td>\n",
       "      <td>0.309997</td>\n",
       "      <td>0.240006</td>\n",
       "      <td>0.560005</td>\n",
       "      <td>-0.049995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-16</th>\n",
       "      <td>124.760002</td>\n",
       "      <td>125.209999</td>\n",
       "      <td>124.330002</td>\n",
       "      <td>124.889999</td>\n",
       "      <td>124.183332</td>\n",
       "      <td>120.958000</td>\n",
       "      <td>132.270333</td>\n",
       "      <td>0.561437</td>\n",
       "      <td>0.449997</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.580002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close         S_3  \\\n",
       "Date                                                                     \n",
       "2013-07-10  121.150002  122.349998  120.589996  120.949997  119.406667   \n",
       "2013-07-11  124.260002  124.360001  123.470001  124.239998  120.360001   \n",
       "2013-07-12  123.519997  124.300003  123.320000  124.129997  121.936666   \n",
       "2013-07-15  124.080002  124.389999  123.839996  124.180000  123.106664   \n",
       "2013-07-16  124.760002  125.209999  124.330002  124.889999  124.183332   \n",
       "\n",
       "                  S_15        S_60      Corr     Std_U     Std_D        OD  \\\n",
       "Date                                                                         \n",
       "2013-07-10  122.149334  132.900000 -0.042512  1.199996  0.560006  0.310006   \n",
       "2013-07-11  121.404000  132.727334 -0.680947  0.099999  0.790001  3.110000   \n",
       "2013-07-12  120.980667  132.584667 -0.449336  0.780006  0.199997 -0.740005   \n",
       "2013-07-15  121.016000  132.439000  0.401985  0.309997  0.240006  0.560005   \n",
       "2013-07-16  120.958000  132.270333  0.561437  0.449997  0.430000  0.680000   \n",
       "\n",
       "                  OL  \n",
       "Date                  \n",
       "2013-07-10  0.529999  \n",
       "2013-07-11  3.310005  \n",
       "2013-07-12 -0.720001  \n",
       "2013-07-15 -0.049995  \n",
       "2013-07-16  0.580002  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "gold_prices = pd.read_csv(\n",
    "    '../data_modules/data_preprocess.csv', index_col='Date')\n",
    "\n",
    "# Print the data\n",
    "gold_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191439f7",
   "metadata": {
    "id": "191439f7"
   },
   "source": [
    "<a id='xy'></a>\n",
    "## Create Input and Output Variables\n",
    " \n",
    "As done previously, we will create an input dataset `X` which is the independent variable and output datasets `yU` and `yD` which are the dependent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dcec15",
   "metadata": {
    "id": "e9dcec15"
   },
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = gold_prices[['Open', 'S_3', 'S_15', 'S_60', 'OD', 'OL', 'Corr']]\n",
    "\n",
    "# Dependent variable for upward deviation\n",
    "yU = gold_prices['Std_U']\n",
    "\n",
    "# Dependent variable for downward deviation\n",
    "yD = gold_prices['Std_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6648cc",
   "metadata": {
    "id": "7a6648cc"
   },
   "source": [
    "<a id='preprocess'></a>\n",
    "## Data Preprocessing and Hyperparameters\n",
    "Feeding the model with preprocessed data in a machine learning model is essential. Raw data contains many errors, and using such data will result in inconsistent and erroneous results. \n",
    "### Pipeline\n",
    "We have already discussed how to create a pipeline and use it on a sample dataset. We will now implement it on our GLD dataset. \n",
    "We are using the following two steps in our pipeline: \n",
    "1. Scale the data\n",
    "2. Fit the data using the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280dcb67",
   "metadata": {
    "id": "280dcb67"
   },
   "outputs": [],
   "source": [
    "# First we put scaling and then linear regression in the pipeline.\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('linear', LinearRegression())]\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed861f2",
   "metadata": {
    "id": "ff114257"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "As mentioned earlier, there are some parameters that the model itself cannot estimate. These values can not be learned from the training data but are fixed when we begin to define our model. They play a crucial role in increasing the performance of the system. Such parameters are called hyperparameters. \n",
    "\n",
    "The number of hyperparameters and the value it can take depends on the kind of ML model we select. Some hyperparameters can take a range of values while others have discrete and specific outcomes. In the case of linear regression, only the intercept can be used as a hyperparameter. \n",
    "\n",
    "We will use the `fit_intercept` function which can tell us whether to calculate the intercept for this model or not. This is a boolean function and hence can only return 0 or 1. If our result is 0, it means that the model performs better without the intercept. If the result is 1, it means an intercept needs to be modelled for better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643fb02c",
   "metadata": {
    "id": "643fb02c"
   },
   "outputs": [],
   "source": [
    "# Here we are using intercept as hyperparameter\n",
    "parameters = {'linear__fit_intercept': [0, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd5e25",
   "metadata": {},
   "source": [
    "<a id='split'></a>\n",
    "## Split Train and Test Data\n",
    "In the train-test split you divide the data into two parts.\n",
    "\n",
    "If we consider the train-test split as 80%-20%, it means 80% of the original data is the training data and the remaining 20% is the testing data. \n",
    "\n",
    "The 80%-20% proportion is a popular proportion to split the data. But there is no rule of thumb that we always have to use the 80%-20% ratio. You can also try other popular proportion choices like 90%-10%, 75%-25%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5ce85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using 80%-20% split, therefore splitting ratio will be 0.80\n",
    "splitting_ratio = .80\n",
    "\n",
    "# Split the data into two parts\n",
    "# Use int to ensure that result is of integer data type.\n",
    "split = int(splitting_ratio*len(gold_prices))\n",
    "\n",
    "# Define train dataset\n",
    "X_train = X[:split]\n",
    "yU_train = yU[:split]\n",
    "yD_train = yD[:split]\n",
    "\n",
    "# Define test data\n",
    "X_test = X[split:]\n",
    "yU_test = yU[split:]\n",
    "yD_test = yD[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed078dd",
   "metadata": {
    "id": "0ed078dd"
   },
   "source": [
    "<a id='cross'></a>\n",
    "## Grid Search Cross-Validation\n",
    "\n",
    "Now that we know what are hyperparameters, our goal is to find hyperparameter values which give the best performance for our model. This is known as hyperparameter tuning.  \n",
    "\n",
    "But the question arises, <b>how to find these best sets of hyperparameters?</b>\n",
    "\n",
    "With respect to linear regression, we will try to understand whether our model is more accurate with an intercept or without an intercept. For this, we will use the Grid Search technique. Grid Search will take the `fit_intercept` value as 0, then 1, and tell us which value is better by calculating the performance for each of them.    \n",
    "\n",
    "However, we cannot perform Grid Search on the whole dataset as it can lead to overfitting. An overfitted model will fit training data very well but it will perform poorly on an unseen dataset. To overcome this problem we will use cross-validation. In this, the dataset is divided into training set, validation set and test set. Essentially the model learns on the training set, evaluates on the validation set and then finally tests on the test set. \n",
    "\n",
    "We will use the `GridSearchCV` function which is an inbuilt function for cross-validation. We have set `cv=5`, which implies that the grid search will consider five rounds of cross-validation for averaging the performance results. We are using `GridSearchCV` instead of `RandomSearchCV` due to fewer features.`TimeSeriesSplit` splits training data into multiple segments. We will also use `neg_mean_squared_error` as a scoring metric for the `GridSearchCV` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a96a22",
   "metadata": {
    "id": "f3a96a22"
   },
   "outputs": [],
   "source": [
    "# Use TimeSeriesSplit for cross validation\n",
    "my_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define reg as variable for GridSearch function containing pipeline, hyperparameters\n",
    "reg = GridSearchCV(pipeline, parameters,\n",
    "                   scoring='neg_mean_squared_error', cv=my_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ac554",
   "metadata": {},
   "source": [
    "We call the `fit` function of the model and pass the `X_train` and `yU_train` datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea9ca6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('linear', LinearRegression())]),\n",
       "             param_grid={'linear__fit_intercept': [0, 1]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "reg.fit(X_train, yU_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0c6b6-c03a-49f5-8dfd-e48a8a31fe41",
   "metadata": {},
   "source": [
    "<a id='perf'></a>\n",
    "## Grid Search Performance \n",
    "\n",
    "<a id='best'></a>\n",
    "### Best Fit Variable \n",
    "\n",
    "Next, we will create the best fit variable by calling the `best_params` function  The `best_params` function will use the best parameter set found by the Grid Search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50035aff-1bdd-4d10-a68b-f992bdcc00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear__fit_intercept': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialise a best fit variable\n",
    "best_fit = reg.best_params_\n",
    "\n",
    "# Print best parameter\n",
    "print(best_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249e6be",
   "metadata": {},
   "source": [
    "We can see that `best_params_` for our model gives `linear_fit_intercept` equal to 1. This means our model will contain an intercept for better performance. \n",
    "\n",
    "Let us also see how the 5 rounds of cross-validation performed on the training data by calculating the mean squared error for each round. We will also see the final score for the `GridSearchCV` method using the `neg_mean_squared_error` scoring method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dbbc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55125259 -0.35420839 -0.27102945 -0.14834363 -0.14452836]\n",
      "-0.24203918499284677\n"
     ]
    }
   ],
   "source": [
    "# Find the scores for 5 rounds of cross-validation\n",
    "CV_scores = cross_val_score(reg, X_train, yU_train, cv=5,\n",
    "                            scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print the scores for 5 rounds of cross-validation\n",
    "print(CV_scores)\n",
    "\n",
    "# Create a score variable\n",
    "score = reg.best_score_\n",
    "\n",
    "# Print the score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3db6e",
   "metadata": {},
   "source": [
    "We can see the final scores of our Grid Search technique above. But all the values are negative. Moreover, they have a lower magnitude as well. This is because we are using `neg_mean_squared_error` which means negated MSE. Since, we want low MSE in our models, functions ending with `_error` or `_loss` in the `sklearn` library return a value that is to be minimised. Hence, the lower the score, the better the result.\n",
    "So, the mean cross-validated score comes out to be around 24% for the training dataset. \n",
    "\n",
    "## Conclusion\n",
    "In this notebook, you learned about hyperparameters and how to find optimal parameters for our model using Grid Search and cross-validation. You also learned how to split our dataset into train and test datasets <br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing and Prediction.ipynb",
   "provenance": [
    {
     "file_id": "1oE_u21fmFtV9vYYfPBMW3rjuH629d5rw",
     "timestamp": 1644837511413
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5159781d4911915b18c0fbe8a046748af728f2e0f1da92475ba775542874e153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
